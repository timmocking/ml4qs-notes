# Lecture 6: Predictive modelling with time

## Time series

Time series focuses on understanding **periodicity and trends**. This can be used for **forecasting** and **control** (how can we influence time series and what is the result).

Time series data can be decomposed into three components:

* Periodic variations (daily, weekly etc.)
* Trend (how does the mean evolve over time)
* Irregular variations (left over after remove periodic variations and trends)



An important aspect of time series data is **stationarity**. Stationarity means that the trends and periodic variations are removed, and that the variance of the remaining residuals are constant.

Often learning algorithms require the data to be stationary. 



**Lagged autocorrelation**

Lagged autocorrelation checks what the correlation is for one point and another shifted by λ. A constant lagged autocorrelation is also an assumption which must be met by time series models.

![](https://latex.codecogs.com/gif.latex?r_%7B%5Clambda%7D%20%3D%20%5Cfrac%7B%5Csum_%7Bt%3D1%7D%5E%7BN-%5Clambda%7D%28x_%7Bt%7D-x%5Cbar%7B%7D%29%28x_%7Bt&plus;%5Clambda%7D-x%5Cbar%7B%7D%29%7D%7B%5Csum_%7Bt%3D1%7D%5E%7BN%7D%28x_%7Bt%7D-x%5Cbar%7B%7D%29%5E2%7D)



**Filtering and smoothing**

We can look at q points in the past and future and define a new point for *x<sub>i</sub>* as follows:

![](https://latex.codecogs.com/gif.latex?z_%7Bt%7D%20%3D%20%5Csum_%7Br%3D-q%7D%5E%7Bq%7Da_%7Br%7Dx_%7Bt&plus;r%7D)

Consider the points (2, 1, 2) and a value *a<sub>r</sub>* of 1/3 and a q of 1. We have a new point (1/3  * 2 + 1/3 * 1 + 1/3 * 2) = 5/3. So the middle point changes form 1 to 1.667. 

This effectively creates a *smoothed* time series, as we reduce the extremes in the time series. 

By changing *a<sub>r</sub>* we can change the measure in which the data is smoothed across the time series. 

One example is **exponential smoothing**.



**De-trending**

We can de-trend (and move to stationarity) the data using the following simple filter: 

![](https://latex.codecogs.com/gif.latex?z_%7Bt%7D%20%3D%20x_%7Bt%7D%20-%20x_%7Bt-1%7D%20%3D%20%5Cbigtriangledown%20t)

We can apply this operator d times.

However *x<sub>-1</sub>* might not always be representative for the trend. Therefore we can also deduct the result of the exponential smoothing (*z<sub>t-1</sub>*) from *x<sub>t</sub>*



### ARIMA

ARIMA takes one feature and uses the historical value for this feature in order to predict the future.

We assume that a measurement is generated by a probability distribution *P<sub>t</sub>* at a given time. 

For this distribution, we can compute the mean and the **auto-covariance**. 

A series is stationary when the mean is constant and the auto-covariance only depends on the time difference λ.

The **auto-regression component** consists of *P<sub>t</sub>* as the sum of the distributions of previous points. This is called the parameter p.

The **moving-average component** consists  of the noise *W<sub>t</sub>*. This is called the parameter q.

We also have a parameter d which defines the differencing.

**Seasonality decomposition **is one way to improve ARIMA models.



## Recurrent neural networks

Recurrent neural networks are variants of neural networks that can take time into account explicitly. 

This is feeding back the output of the network into the input layer, thus creating a sort of "memory". 

However, this creates some problems with backpropagation. 

Backpropagation in RNNs is done by unfolding the network (looking at the network as a series of networks) .



### LSTMs

LSTMs are a special kind of RNN that are better at learning long-term patterns.

LSTMs are the most popular RNNs nowadays.



### Echo state networks

Training RNNs is difficult in practice.

Echo state networks have a reservoir of randomly collected neurons (with varying connections). 

There are 3 weight matrices: *W<sub>in</sub>*, *W*, *W<sub>out.</sub>*.

*W<sub>in</sub>*,  and *W* are randomly set and aren't updated during training. The only thing that is learned is *W<sub>out</sub>*. The underlying assumption is that the right signals will come inevitably come out with a large enough reservoir. 

This random reservoir should satisfy the **echo state property** which means that the effect of a previous state and the previous input on a future state should vanish gradually as time passes. 

Reservoirs can initialized with the **reservoir initialization procedure**. This tells us how to build a matrix of weight that complies with the echo state property. 



## Dynamical systems models

Dynamical systems model assume that we have a set of differential equations between attributes and targets.

These equations are based on domain knowledge, but still contain parameters that can be tuned. 



## Methods of parameter optimization

### Simulated annealing

Simulated annealing is a simple algorithm for finding optimal parameters. 

We randomly pick possible moves in the parameter spaces and go to a new position if it decreases the error.

However, we also move to positions that are worse, with a certain probability. 

Our parameter space has a *temperature* which decreases over time as well as a maximum number of iterations. The temperature defines our willingness to go to worse vectors. 



### Genetic algorithms

We create a population of parameter vectors. 

From this population, we create a new population where the chances of being picked are higher for vectors that have a low error.

 From the "winners", we create a new population  (a mating pool) using crossover and mutation. We continue this for a number of generations.



### Multi-criteria optimization

Sometimes we have multiple targets (e.g. you want to predict both activity level and the mood). This is a **multi-criteria optimization problem**. 

One parameter setting might be good for one target, while another one works well for the other. 



**Pareto front**

We can plot the error for both targets for all parameter vectors in one plot. This is called the pareto front. 

We call some model instance **dominated** if its error is higher for one of the targets compared to the other target(s) of another model instance. 

Ideally, we want to find non-dominated model instances. 



**NSGA-II**

NSGA-II is a variation of the genetic algorithm which can find non-dominated model instances by creating fronts from the population. 

Ideally, you want to get a good spread of solutions across the pareto front. You then sort the points on the front based on distance to other points. We then create a new population based on these distances. 
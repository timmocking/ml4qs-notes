# Lecture 2: Handling sensory noise

## Removal of outliers

An outlier is an observation point that is distant from other observations.

This could be caused by:

* Measurement errors
* Variability (e.g. someone pushing their limits)

These two sources of outliers are generated by different mechanisms. 

We can remove these using domain knowledge (e.g. heart rate cannot be over 220) or without domain knowledge. 



## Distribution based outlier detection

### Chauvenet's criterion

>  The idea behind Chauvenet's criterion is to find a probability band, centered on the mean of a normal distribution, that should reasonably contain all n samples of a data set.

Using this criterion, we can try to fit a normal distribution to the data and define outliers based on their position on the tails of this distribution.

For each instance *i* for attribute *j* we can compute the probability of the observation under the normal distribution:

![](https://latex.codecogs.com/gif.latex?P(X&space;\leq&space;x_{ij})&space;<&space;\frac{1}{cN})

A parameter *c* defines the cut-off point for when something is called an outlier. 

Intuitively, this means that a larger *c* increases the denominator, decreasing the amount of instances being called as outliers. 



### Mixture models

Mixture models assume that the data can be described with K normal distributions.

We find the best fit for the parameters by means of **maximizing** the **likelihood** of a distribution given the data.

This can be done using the expectation-maximization algorithm (EM).



### Distance based outlier detection

Let *d*(X<sub>i</sub>, X<sub>j</sub>) represent the distance between two values of attribute *j*.

Points can be defined as outliers when there are more values outside of range of distance (d<sub>min</sub>)  than inside according to a fraction f<sub>min</sub>. 

However, this approach does not take local density into account (imagine two clusters far away from each other).



### Local outlier factor

Local outlier factor compares the local density of a point with the densities of its neighbors. 

Define a distance *k*<sub>dist</sub> for a point as the largest distance to one of its k closest neighbours. 

All neighbours closer than *k*<sub>distÂ </sub>belong to the *k-distance neighbourhood*  (k<sub>dist_nh</sub>) 

Define a reachability between two points A and B as follows:

![](https://latex.codecogs.com/gif.latex?K_{reach,dist}(A,B)=&space;max(k_{dist,&space;B},&space;d(A,B))) 

This is then compared to the reachability of neighbours, to define *local reachability* (LOF). 

If LOF is ~1, then there is a similar density between a point and its neighbours. If a point has a lower density than its neighbours (LOF > 1), then it is more likely to be an outlier.

For more information, see the [wikipedia page](https://en.wikipedia.org/wiki/Local_outlier_factor).



## Imputation of missing values

Outliers can be removed, which leads to missing values. Naturally, most dataset also contain missing values from themselves.

How can we impute missing values?

* Mean (numeric)
* Mode (categorical and numeric)
* Median (numeric)

Often this does not make sense. For example with time series data such as the crowdsignals data, we would rather take surrounding values in the neighbourhood into consideration.

More advanced approaches utilize either attribute values in the same instance or values of the same attributes from other instances (only applicable for ordered/temporal attributes).

An example of this could be k-nearest neighbour imputation (take the average of your k-neighbours). 



### Kalman filters

The Kalman filter estimates expected values based on historical data, and if the observed value deviates too much from this expectation (i.e. an outlier), we can impute this with the expected value. 

Kalman filters are thus a combination of outlier detection and imputation.

We have a latent state s<sub>t</sub> for which our quantified self data performs measurements. We have the position and velocity (direction) of past measurements, and we can calculate where the next value is likely to be, while incorporating some noise.

These are essentially predictions.

<img src="https://www.bzarg.com/wp-content/uploads/2015/08/gauss_9.jpg" alt="gauss_9" style="zoom:40%;" align="left"/>



## Finding most useful information

After imputation and removing outliers, there is often still residual noise in the data.

Various techniques exist to reduce this noise even further, such as lowpass filters and principal component analysis. 



###  Lowpass filter

If data has periodicity, series of values can be decomposed into different periodic signals (e.g. walking: 1 Hz, jogging: 2 Hz) with their own frequencies. 

Using lowpass filters, you can filter out higher frequency data (as opposed to the highpass filter, which filters out lower frequency data). This is done using a cut-off frequency, where frequencies higher than the cut-off will be removed to a certain extent.



### Principal component analysis

This technique finds features that explain most of the variance in the data, which reduces noise to the background.

